
# openai, streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import openai
import streamlit as st
import os
import warnings

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA, ConversationalRetrievalChain

from langchain.memory import ConversationBufferMemory

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")


openai_api_key = os.getenv("OPENAI_API_KEY")
k = 3

# ğŸ’¬ ì•± ì œëª©ê³¼ ğŸš€ ì„¤ëª…
st.title("ğŸ’¬ ì±—ë´‡")
st.caption("ğŸš€ OpenAIë¥¼ ì´ìš©í•œ ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë´‡")

# ì´ˆê¸° ë©”ì‹œì§€ ì„¤ì •
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]
    

    db_path = '../db3'
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    st.session_state["database"] = Chroma(persist_directory= db_path, embedding_function = embeddings )
    
    retriever =  st.session_state["database"].as_retriever(search_kwargs={"k": k})
    chat = ChatOpenAI(model="gpt-3.5-turbo")
    st.session_state["memory"] = ConversationBufferMemory(memory_key="chat_history", input_key="question",
                                output_key="answer", return_messages=True)
    st.session_state["qa"] = ConversationalRetrievalChain.from_llm(llm=chat, retriever=retriever, memory=st.session_state["memory"],    
                                           return_source_documents=True,  output_key="answer")

# ëª¨ë“  ëŒ€í™” ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œ
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])


# ì‚¬ìš©ì ì±„íŒ… ì…ë ¥ í™•ì¸
if prompt := st.chat_input():

    # API í‚¤ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°, ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê³  í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨
    if not openai_api_key:
        st.info("OpenAI API keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    # OpenAI API í‚¤ ì„¤ì •
    openai.api_key = openai_api_key

    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    
    result = st.session_state["qa"]({"question": prompt})
    msg = result["answer"]

    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)

    # í˜„ì¬ ë‹´ê²¨ ìˆëŠ” ë©”ëª¨ë¦¬ ë‚´ìš© ì „ì²´ í™•ì¸
    history = st.session_state["memory"].load_memory_variables({})
    print(history)

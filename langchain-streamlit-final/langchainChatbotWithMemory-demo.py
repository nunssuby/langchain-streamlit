import openai
import streamlit as st
import os
import warnings

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA, ConversationalRetrievalChain

from langchain.memory import ConversationBufferMemory

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
k = 3

# ğŸ’¬ ì•± ì œëª©ê³¼ ğŸš€ ì„¤ëª…
st.title("ğŸ’¬ ì±—ë´‡")
st.caption("ğŸš€ OpenAIë¥¼ ì´ìš©í•œ ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë´‡")

# ì´ˆê¸° ë©”ì‹œì§€ ì„¤ì •
if "conversations" not in st.session_state:
    st.session_state["conversations"] = []
    st.session_state["current_conversation"] = []
    st.session_state["selected_conversation"] = None
    
    db_path = '../db3'
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    st.session_state["database"] = Chroma(persist_directory=db_path, embedding_function=embeddings)
    
    retriever = st.session_state["database"].as_retriever(search_kwargs={"k": k})
    chat = ChatOpenAI(model="gpt-3.5-turbo")
    st.session_state["memory"] = ConversationBufferMemory(memory_key="chat_history", input_key="question",
                                                          output_key="answer", return_messages=True)
    st.session_state["qa"] = ConversationalRetrievalChain.from_llm(llm=chat, retriever=retriever, memory=st.session_state["memory"],
                                                                    return_source_documents=True, output_key="answer")

# ì‚¬ì´ë“œë°”ì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ìƒˆ ëŒ€í™” ë²„íŠ¼ ì¶”ê°€
with st.sidebar:
    st.header("ëŒ€í™” ê¸°ë¡")
    
    # ìƒˆ ëŒ€í™” ë²„íŠ¼ ì¶”ê°€
    if st.button("ìƒˆ ëŒ€í™” ì‹œì‘"):
        st.session_state["current_conversation"] = []
        st.session_state["selected_conversation"] = None

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for i, conversation in enumerate(st.session_state["conversations"]):
        if conversation:  # ëŒ€í™”ì— ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
            if st.button(conversation[0]["content"], key=f"conv_{i}"):
                st.session_state["selected_conversation"] = i
                st.session_state["current_conversation"] = conversation

# ì„ íƒëœ ëŒ€í™” í‘œì‹œ
if st.session_state["current_conversation"]:
    for msg in st.session_state["current_conversation"]:
        st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì±„íŒ… ì…ë ¥ í™•ì¸
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):

    # API í‚¤ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°, ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ê³  í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨
    if not openai_api_key:
        st.info("OpenAI API keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ í˜„ì¬ ëŒ€í™”ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state["current_conversation"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ì²˜ìŒ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œ ëŒ€í™” ëª©ë¡ì— ì¶”ê°€
    if len(st.session_state["current_conversation"]) == 1:
        st.session_state["conversations"].append(st.session_state["current_conversation"])
        st.session_state["selected_conversation"] = len(st.session_state["conversations"]) - 1

    # ConversationalRetrievalQA ì²´ì¸ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    result = st.session_state["qa"]({"question": prompt})
    msg = result["answer"]

    # ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì„ í˜„ì¬ ëŒ€í™”ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state["current_conversation"].append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)

    # ëŒ€í™” ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ì´ì–´ì§„ ëŒ€í™”ë¥¼ ë°˜ì˜
    st.session_state["conversations"][st.session_state["selected_conversation"]] = st.session_state["current_conversation"]

    # í˜„ì¬ ë‹´ê²¨ ìˆëŠ” ë©”ëª¨ë¦¬ ë‚´ìš© ì „ì²´ í™•ì¸
    history = st.session_state["memory"].load_memory_variables({})
    print(history)
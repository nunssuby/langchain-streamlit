import openai
import streamlit as st
import os
import warnings

from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("OpenAI API keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Secretsì— API keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

k = 3

# ğŸ’¬ ì•± ì œëª©ê³¼ ğŸš€ ì„¤ëª…
st.title("ğŸ’¬ ì±—ë´‡")
st.caption("ğŸš€ OpenAIë¥¼ ì´ìš©í•œ ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë´‡")

# ì´ˆê¸° ë©”ì‹œì§€ ì„¤ì •
if "conversations" not in st.session_state:
    st.session_state["conversations"] = []
    st.session_state["current_conversation"] = []
    st.session_state["selected_conversation"] = None
    
    db_path = '../aivle_db'
    embeddings = None  # ì´ˆê¸°í™”

    # OpenAI Embeddings ì´ˆê¸°í™”
    try:
        embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002", 
            openai_api_key=openai_api_key
        )
    except Exception as e:
        st.error(f"Embeddings ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    try:
        st.session_state["database"] = Chroma(persist_directory=db_path, embedding_function=embeddings)
    except Exception as e:
        st.error(f"Chroma ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    st.session_state["retriever"] = st.session_state["database"].as_retriever(search_kwargs={"k": k})
    st.session_state["chat_model"] = ChatOpenAI(model="gpt-3.5-turbo")

# ìƒë‹¨ ì˜¤ë¥¸ìª½ì— ìƒˆ ëŒ€í™” ë²„íŠ¼ì„ ë°°ì¹˜
col1, col2 = st.columns([3, 1])

with col2:
    if st.button("ìƒˆ ëŒ€í™” ì‹œì‘", key="new_conv", help="ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤"):
        st.session_state["current_conversation"] = []
        st.session_state["selected_conversation"] = len(st.session_state["conversations"])
        new_memory = ConversationBufferMemory(memory_key="chat_history", input_key="question",
                                              output_key="answer", return_messages=True)
        st.session_state["conversations"].append({"messages": [], "memory": new_memory})

# ì‚¬ì´ë“œë°”ì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
with st.sidebar:
    st.header("ëŒ€í™” ê¸°ë¡")
    
    for i, conversation in enumerate(st.session_state["conversations"]):
        if conversation["messages"]:
            if st.button(conversation["messages"][0]["content"], key=f"conv_{i}"):
                st.session_state["selected_conversation"] = i
                st.session_state["current_conversation"] = conversation["messages"]

# ì„ íƒëœ ëŒ€í™” í‘œì‹œ
if st.session_state["current_conversation"]:
    for msg in st.session_state["current_conversation"]:
        st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì±„íŒ… ì…ë ¥ í™•ì¸
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):

    if not openai_api_key:
        st.info("OpenAI API keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    if st.session_state["selected_conversation"] is None:
        st.session_state["selected_conversation"] = len(st.session_state["conversations"])
        new_memory = ConversationBufferMemory(memory_key="chat_history", input_key="question",
                                              output_key="answer", return_messages=True)
        st.session_state["conversations"].append({"messages": [], "memory": new_memory})

    st.session_state["current_conversation"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    st.session_state["conversations"][st.session_state["selected_conversation"]]["messages"] = st.session_state["current_conversation"]

    current_memory = st.session_state["conversations"][st.session_state["selected_conversation"]]["memory"]

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=st.session_state["chat_model"], 
        retriever=st.session_state["retriever"], 
        memory=current_memory,
        return_source_documents=True, 
        output_key="answer"
    )

    result = qa_chain({"question": prompt})
    msg = result["answer"]

    st.session_state["current_conversation"].append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)

    st.session_state["conversations"][st.session_state["selected_conversation"]]["messages"] = st.session_state["current_conversation"]

    history = current_memory.load_memory_variables({})
    print(history)